{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd968b5d-b90b-4ad7-a46f-099ab0e6c8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/conda/lib/python3.11/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.11/site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/lib/python3.11/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Requirement already satisfied: imblearn in /opt/conda/lib/python3.11/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.11/site-packages (from imblearn) (0.12.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "Requirement already satisfied: dask[dataframe] in /opt/conda/lib/python3.11/site-packages (2024.2.1)\n",
      "Requirement already satisfied: click>=8.1 in /opt/conda/lib/python3.11/site-packages (from dask[dataframe]) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from dask[dataframe]) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /opt/conda/lib/python3.11/site-packages (from dask[dataframe]) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from dask[dataframe]) (24.1)\n",
      "Requirement already satisfied: partd>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from dask[dataframe]) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.11/site-packages (from dask[dataframe]) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from dask[dataframe]) (0.12.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /opt/conda/lib/python3.11/site-packages (from dask[dataframe]) (6.10.0)\n",
      "Requirement already satisfied: pandas>=1.3 in /opt/conda/lib/python3.11/site-packages (from dask[dataframe]) (1.5.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->dask[dataframe]) (3.20.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.3->dask[dataframe]) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.3->dask[dataframe]) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.3->dask[dataframe]) (1.26.4)\n",
      "Requirement already satisfied: locket in /opt/conda/lib/python3.11/site-packages (from partd>=1.2.0->dask[dataframe]) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas>=1.3->dask[dataframe]) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "!pip install imblearn\n",
    "!pip install dask[dataframe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25961a52-9daa-4f72-956a-6d0ced853d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from SQLite database...\n",
      "Data Loaded Successfully.\n",
      "Number of matches: 25979\n",
      "Number of teams: 299\n",
      "Number of players: 11060\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# ================================\n",
    "# Data Loading\n",
    "# ================================\n",
    "\n",
    "print(\"Loading data from SQLite database...\")\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('./database.sqlite')\n",
    "\n",
    "# Load tables into pandas DataFrames\n",
    "country = pd.read_sql_query(\"SELECT * FROM Country\", conn)\n",
    "league = pd.read_sql_query(\"SELECT * FROM League\", conn)\n",
    "match = pd.read_sql_query(\"SELECT * FROM Match\", conn)\n",
    "player = pd.read_sql_query(\"SELECT * FROM Player\", conn)\n",
    "player_attributes = pd.read_sql_query(\"SELECT * FROM Player_Attributes\", conn)\n",
    "team = pd.read_sql_query(\"SELECT * FROM Team\", conn)\n",
    "team_attributes = pd.read_sql_query(\"SELECT * FROM Team_Attributes\", conn)\n",
    "\n",
    "print(\"Data Loaded Successfully.\")\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Number of matches: {match.shape[0]}\")\n",
    "print(f\"Number of teams: {team.shape[0]}\")\n",
    "print(f\"Number of players: {player.shape[0]}\")\n",
    "\n",
    "# ================================\n",
    "# Data Preprocessing and Merging\n",
    "# ================================\n",
    "\n",
    "# Merge country and league data into match data\n",
    "match = match.merge(country, how='left', left_on='country_id', right_on='id', suffixes=('', '_country'))\n",
    "match = match.merge(league, how='left', left_on='league_id', right_on='id', suffixes=('', '_league'))\n",
    "\n",
    "# Rename columns for clarity\n",
    "match.rename(columns={'name': 'country_name', 'name_league': 'league_name'}, inplace=True)\n",
    "\n",
    "# Convert date columns to datetime\n",
    "match['date'] = pd.to_datetime(match['date'])\n",
    "team_attributes['date'] = pd.to_datetime(team_attributes['date'])\n",
    "player_attributes['date'] = pd.to_datetime(player_attributes['date'])\n",
    "player['birthday'] = pd.to_datetime(player['birthday'])\n",
    "\n",
    "# Create match outcome variable\n",
    "# 1 for home win, 0 for draw, -1 for away win\n",
    "def get_match_result(row):\n",
    "    if row['home_team_goal'] > row['away_team_goal']:\n",
    "        return 1  # Home win\n",
    "    elif row['home_team_goal'] == row['away_team_goal']:\n",
    "        return 0  # Draw\n",
    "    else:\n",
    "        return -1  # Away win\n",
    "\n",
    "match['match_result'] = match.apply(get_match_result, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de7bca98-75d0-4e94-b38a-419b6e89ab2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting team attributes...\n",
      "Extracting player attributes (this may take a while)...\n",
      "Calculating recent team performance...\n",
      "Calculating head-to-head statistics...\n",
      "Cleaning data by dropping rows with missing values in selected features...\n",
      "Data size before cleaning: 25979 rows\n",
      "Data size after cleaning: 19355 rows\n",
      "Handling class imbalance using SMOTE...\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Feature Engineering\n",
    "# ================================\n",
    "\n",
    "# --- Team Attributes ---\n",
    "\n",
    "# Define the columns to use from team_attributes\n",
    "team_attribute_cols = [\n",
    "    'buildUpPlaySpeed', 'buildUpPlayPassing', 'chanceCreationPassing',\n",
    "    'chanceCreationCrossing', 'chanceCreationShooting', 'defencePressure',\n",
    "    'defenceAggression', 'defenceTeamWidth', 'buildUpPlayPositioningClass',\n",
    "    'chanceCreationPositioningClass', 'defenceDefenderLineClass'\n",
    "]\n",
    "\n",
    "# Prepare team attributes by selecting the most recent attributes before the match date\n",
    "def get_team_attributes(team_id, match_date):\n",
    "    attributes = team_attributes[team_attributes['team_api_id'] == team_id]\n",
    "    attributes = attributes[attributes['date'] <= match_date]\n",
    "    attributes = attributes.sort_values(by='date', ascending=False)\n",
    "    if not attributes.empty:\n",
    "        attributes = attributes.iloc[0]\n",
    "        return attributes[team_attribute_cols]\n",
    "    else:\n",
    "        # Return a Series with NaNs and correct index\n",
    "        return pd.Series([np.nan]*len(team_attribute_cols), index=team_attribute_cols)\n",
    "\n",
    "# Apply the function to get team attributes for home and away teams\n",
    "print(\"Extracting team attributes...\")\n",
    "home_team_attrs = []\n",
    "away_team_attrs = []\n",
    "\n",
    "for idx, row in match.iterrows():\n",
    "    home_attrs = get_team_attributes(row['home_team_api_id'], row['date'])\n",
    "    away_attrs = get_team_attributes(row['away_team_api_id'], row['date'])\n",
    "    home_team_attrs.append(home_attrs)\n",
    "    away_team_attrs.append(away_attrs)\n",
    "\n",
    "home_team_attrs_df = pd.DataFrame(home_team_attrs)\n",
    "away_team_attrs_df = pd.DataFrame(away_team_attrs)\n",
    "\n",
    "# Add prefixes to columns to distinguish between home and away attributes\n",
    "home_team_attrs_df = home_team_attrs_df.add_prefix('home_')\n",
    "away_team_attrs_df = away_team_attrs_df.add_prefix('away_')\n",
    "\n",
    "# Concatenate the attributes to the match data\n",
    "match = pd.concat([match.reset_index(drop=True), home_team_attrs_df.reset_index(drop=True), away_team_attrs_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Check for duplicate columns\n",
    "duplicate_columns = match.columns[match.columns.duplicated()]\n",
    "if duplicate_columns.any():\n",
    "    print(f\"Duplicate columns in match DataFrame: {duplicate_columns}\")\n",
    "\n",
    "# --- Player Attributes ---\n",
    "\n",
    "# Function to get the average player attributes for a team before a match\n",
    "def get_team_player_attributes(team_id, match_date, player_cols, match_row, team_type):\n",
    "    # Get player IDs from the team's lineup in the match\n",
    "    players = []\n",
    "    for i in range(1, 12):\n",
    "        player_id = match_row.get(f'{team_type}_player_{i}', None)\n",
    "        if not pd.isnull(player_id):\n",
    "            players.append(player_id)\n",
    "    # Get the most recent attributes for each player\n",
    "    player_attrs = player_attributes[player_attributes['player_api_id'].isin(players)]\n",
    "    player_attrs = player_attrs[player_attrs['date'] <= match_date]\n",
    "    player_attrs = player_attrs.sort_values(by='date', ascending=False).drop_duplicates(subset=['player_api_id'], keep='first')\n",
    "    # Return the average attributes\n",
    "    if not player_attrs.empty:\n",
    "        return player_attrs[player_cols].mean()\n",
    "    else:\n",
    "        return pd.Series([np.nan]*len(player_cols), index=player_cols)\n",
    "\n",
    "print(\"Extracting player attributes (this may take a while)...\")\n",
    "team_player_attrs = []\n",
    "\n",
    "# Define player attributes columns to use\n",
    "player_cols = [\n",
    "    'overall_rating', 'potential', 'crossing', 'finishing', 'heading_accuracy',\n",
    "    'short_passing', 'dribbling', 'long_passing', 'ball_control',\n",
    "    'acceleration', 'sprint_speed', 'agility', 'stamina', 'strength',\n",
    "    'penalties', 'marking', 'standing_tackle', 'sliding_tackle'\n",
    "]\n",
    "\n",
    "# We have expanded the player attributes to include more features\n",
    "\n",
    "for idx, row in match.iterrows():\n",
    "    home_player_attrs = get_team_player_attributes(row['home_team_api_id'], row['date'], player_cols, row, 'home')\n",
    "    away_player_attrs = get_team_player_attributes(row['away_team_api_id'], row['date'], player_cols, row, 'away')\n",
    "    home_player_attrs = home_player_attrs.add_prefix('home_player_')\n",
    "    away_player_attrs = away_player_attrs.add_prefix('away_player_')\n",
    "    combined_attrs = pd.concat([home_player_attrs, away_player_attrs], axis=0)\n",
    "    team_player_attrs.append(combined_attrs)\n",
    "\n",
    "team_player_attrs_df = pd.DataFrame(team_player_attrs)\n",
    "\n",
    "# Concatenate player attributes to match data\n",
    "match = pd.concat([match.reset_index(drop=True), team_player_attrs_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# --- Recent Team Performance ---\n",
    "\n",
    "# Function to calculate team's recent performance before a match\n",
    "def get_recent_performance(team_id, date, n_matches=5):\n",
    "    team_matches_home = match[(match['home_team_api_id'] == team_id) & (match['date'] < date)]\n",
    "    team_matches_away = match[(match['away_team_api_id'] == team_id) & (match['date'] < date)]\n",
    "    team_matches = pd.concat([team_matches_home, team_matches_away]).sort_values(by='date', ascending=False)\n",
    "    team_matches = team_matches.head(n_matches)\n",
    "    if not team_matches.empty:\n",
    "        results = []\n",
    "        for idx, row in team_matches.iterrows():\n",
    "            if row['home_team_api_id'] == team_id:\n",
    "                if row['match_result'] == 1:\n",
    "                    results.append(1)\n",
    "                elif row['match_result'] == 0:\n",
    "                    results.append(0)\n",
    "                else:\n",
    "                    results.append(-1)\n",
    "            else:\n",
    "                if row['match_result'] == -1:\n",
    "                    results.append(1)\n",
    "                elif row['match_result'] == 0:\n",
    "                    results.append(0)\n",
    "                else:\n",
    "                    results.append(-1)\n",
    "        return np.mean(results)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "print(\"Calculating recent team performance...\")\n",
    "home_recent_performance = []\n",
    "away_recent_performance = []\n",
    "\n",
    "for idx, row in match.iterrows():\n",
    "    home_perf = get_recent_performance(row['home_team_api_id'], row['date'])\n",
    "    away_perf = get_recent_performance(row['away_team_api_id'], row['date'])\n",
    "    home_recent_performance.append(home_perf)\n",
    "    away_recent_performance.append(away_perf)\n",
    "\n",
    "match['home_recent_performance'] = home_recent_performance\n",
    "match['away_recent_performance'] = away_recent_performance\n",
    "\n",
    "# --- Head-to-Head Statistics ---\n",
    "\n",
    "# Function to calculate head-to-head statistics\n",
    "def get_head_to_head(home_team_id, away_team_id, date):\n",
    "    h2h_matches = match[((match['home_team_api_id'] == home_team_id) & (match['away_team_api_id'] == away_team_id)) |\n",
    "                        ((match['home_team_api_id'] == away_team_id) & (match['away_team_api_id'] == home_team_id))]\n",
    "    h2h_matches = h2h_matches[h2h_matches['date'] < date].sort_values(by='date', ascending=False)\n",
    "    h2h_matches = h2h_matches.head(5)\n",
    "    if not h2h_matches.empty:\n",
    "        results = []\n",
    "        for idx, row in h2h_matches.iterrows():\n",
    "            if row['home_team_api_id'] == home_team_id and row['match_result'] == 1:\n",
    "                results.append(1)\n",
    "            elif row['away_team_api_id'] == home_team_id and row['match_result'] == -1:\n",
    "                results.append(1)\n",
    "            elif row['match_result'] == 0:\n",
    "                results.append(0)\n",
    "            else:\n",
    "                results.append(-1)\n",
    "        return np.mean(results)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "print(\"Calculating head-to-head statistics...\")\n",
    "h2h_performance = []\n",
    "\n",
    "for idx, row in match.iterrows():\n",
    "    h2h_perf = get_head_to_head(row['home_team_api_id'], row['away_team_api_id'], row['date'])\n",
    "    h2h_performance.append(h2h_perf)\n",
    "\n",
    "match['head_to_head'] = h2h_performance\n",
    "\n",
    "# --- Categorical Encoding ---\n",
    "\n",
    "# Corrected column names (fixed 'defenceDefensiveLineClass' to 'defenceDefenderLineClass')\n",
    "categorical_cols = [\n",
    "    'home_buildUpPlayPositioningClass', 'home_chanceCreationPositioningClass', 'home_defenceDefenderLineClass',\n",
    "    'away_buildUpPlayPositioningClass', 'away_chanceCreationPositioningClass', 'away_defenceDefenderLineClass'\n",
    "]\n",
    "\n",
    "# Replace missing categorical values with a placeholder\n",
    "for col in categorical_cols:\n",
    "    if col in match.columns:\n",
    "        match[col].fillna('Unknown', inplace=True)\n",
    "        match[col] = match[col].astype('category').cat.codes\n",
    "\n",
    "# --- Feature Selection ---\n",
    "\n",
    "# Select relevant features\n",
    "features = [\n",
    "    # Team attributes\n",
    "    'home_buildUpPlaySpeed', 'home_buildUpPlayPassing', 'home_chanceCreationPassing',\n",
    "    'home_chanceCreationCrossing', 'home_chanceCreationShooting', 'home_defencePressure',\n",
    "    'home_defenceAggression', 'home_defenceTeamWidth',\n",
    "    'away_buildUpPlaySpeed', 'away_buildUpPlayPassing', 'away_chanceCreationPassing',\n",
    "    'away_chanceCreationCrossing', 'away_chanceCreationShooting', 'away_defencePressure',\n",
    "    'away_defenceAggression', 'away_defenceTeamWidth',\n",
    "    # Player attributes\n",
    "    'home_player_overall_rating', 'home_player_potential', 'home_player_crossing',\n",
    "    'home_player_finishing', 'home_player_heading_accuracy', 'home_player_short_passing',\n",
    "    'home_player_dribbling', 'home_player_long_passing', 'home_player_ball_control',\n",
    "    'home_player_acceleration', 'home_player_sprint_speed', 'home_player_agility',\n",
    "    'home_player_stamina', 'home_player_strength', 'home_player_penalties',\n",
    "    'home_player_marking', 'home_player_standing_tackle', 'home_player_sliding_tackle',\n",
    "    'away_player_overall_rating', 'away_player_potential', 'away_player_crossing',\n",
    "    'away_player_finishing', 'away_player_heading_accuracy', 'away_player_short_passing',\n",
    "    'away_player_dribbling', 'away_player_long_passing', 'away_player_ball_control',\n",
    "    'away_player_acceleration', 'away_player_sprint_speed', 'away_player_agility',\n",
    "    'away_player_stamina', 'away_player_strength', 'away_player_penalties',\n",
    "    'away_player_marking', 'away_player_standing_tackle', 'away_player_sliding_tackle',\n",
    "    # Recent performance\n",
    "    'home_recent_performance', 'away_recent_performance',\n",
    "    # Head-to-head\n",
    "    'head_to_head',\n",
    "    # Categorical features\n",
    "    'home_buildUpPlayPositioningClass', 'home_chanceCreationPositioningClass', 'home_defenceDefenderLineClass',\n",
    "    'away_buildUpPlayPositioningClass', 'away_chanceCreationPositioningClass', 'away_defenceDefenderLineClass'\n",
    "]\n",
    "\n",
    "# Ensure all features exist in the match DataFrame\n",
    "existing_features = [col for col in features if col in match.columns]\n",
    "missing_features = [col for col in features if col not in match.columns]\n",
    "if missing_features:\n",
    "    print(f\"The following features are missing and will be excluded: {missing_features}\")\n",
    "\n",
    "# Remove any rows with missing values in the features or target variable\n",
    "print(\"Cleaning data by dropping rows with missing values in selected features...\")\n",
    "match_cleaned = match.dropna(subset=existing_features + ['match_result'])\n",
    "print(f\"Data size before cleaning: {match.shape[0]} rows\")\n",
    "print(f\"Data size after cleaning: {match_cleaned.shape[0]} rows\")\n",
    "\n",
    "X = match_cleaned[existing_features]\n",
    "y = match_cleaned['match_result']\n",
    "\n",
    "# Convert target variable to numeric classes (0,1,2)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "print(\"Handling class imbalance using SMOTE...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_scaled, y_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a6e4761-44fd-4823-b125-06d34a25038d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6263669 , -1.16435446,  1.68498525, ...,  0.25880311,\n",
       "         0.37201543, -0.27152687],\n",
       "       [-1.04559822,  0.18672951, -0.71104109, ...,  0.25880311,\n",
       "         0.37201543, -0.27152687],\n",
       "       [ 1.05055838,  1.08745215, -0.23183582, ...,  0.25880311,\n",
       "         0.37201543, -0.27152687],\n",
       "       ...,\n",
       "       [-0.45867437,  0.63709083, -0.71104109, ...,  0.25880311,\n",
       "         0.37201543, -0.27152687],\n",
       "       [ 1.05055838,  1.53781347,  0.05568734, ...,  0.25880311,\n",
       "         0.37201543, -0.27152687],\n",
       "       [ 0.63105855, -0.71226246, -0.23214274, ...,  0.25880311,\n",
       "         0.37201543, -0.27152687]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "589dfa5b-413a-4a05-8120-36b8cab69f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training and test sets...\n",
      "\n",
      "Training and evaluating models...\n",
      "\n",
      "Training and evaluating models...\n",
      "\n",
      "Logistic Regression Cross-validation Accuracy: 0.4634 (+/- 0.0077)\n",
      "Logistic Regression Test Accuracy: 0.4697\n",
      "\n",
      "Support Vector Machine Cross-validation Accuracy: 0.5265 (+/- 0.0075)\n",
      "Support Vector Machine Test Accuracy: 0.5325\n",
      "\n",
      "Random Forest Cross-validation Accuracy: 0.6352 (+/- 0.0054)\n",
      "Random Forest Test Accuracy: 0.6594\n",
      "\n",
      "Gradient Boosting Cross-validation Accuracy: 0.5317 (+/- 0.0060)\n",
      "Gradient Boosting Test Accuracy: 0.5314\n",
      "\n",
      "K-Nearest Neighbors Cross-validation Accuracy: 0.5321 (+/- 0.0085)\n",
      "K-Nearest Neighbors Test Accuracy: 0.5535\n",
      "\n",
      "Neural Network Cross-validation Accuracy: 0.5247 (+/- 0.0045)\n",
      "Neural Network Test Accuracy: 0.5365\n",
      "\n",
      "Extra Trees Cross-validation Accuracy: 0.6597 (+/- 0.0051)\n",
      "Extra Trees Test Accuracy: 0.6906\n",
      "\n",
      "XGBoost Cross-validation Accuracy: 0.5875 (+/- 0.0039)\n",
      "XGBoost Test Accuracy: 0.5910\n",
      "\n",
      "CatBoost Cross-validation Accuracy: 0.5967 (+/- 0.0032)\n",
      "CatBoost Test Accuracy: 0.6110\n",
      "\n",
      "Models ranked by test accuracy:\n",
      "Extra Trees: 0.6906\n",
      "Random Forest: 0.6594\n",
      "CatBoost: 0.6110\n",
      "XGBoost: 0.5910\n",
      "K-Nearest Neighbors: 0.5535\n",
      "Neural Network: 0.5365\n",
      "Support Vector Machine: 0.5325\n",
      "Gradient Boosting: 0.5314\n",
      "Logistic Regression: 0.4697\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Model Training and Evaluation\n",
    "# ================================\n",
    "\n",
    "# Split data into training and test sets\n",
    "print(\"Splitting data into training and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42)\n",
    "\n",
    "# label_mapping = {'D': 0, 'L': 1, 'W': 2}\n",
    "# y_numeric = np.array([label_mapping[label] for label in y])\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df, y_numeric, shuffle=True, random_state=42)\n",
    "# Define models to train\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "    'Support Vector Machine': SVC(probability=True, class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Neural Network': MLPClassifier(max_iter=1000),\n",
    "    'Extra Trees': ExtraTreesClassifier(class_weight='balanced'),\n",
    "    'XGBoost': XGBClassifier(eval_metric='mlogloss'),\n",
    "    # 'LightGBM': LGBMClassifier(force_col_wise=True),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0)\n",
    "}\n",
    "\n",
    "# Train and evaluate models using cross-validation\n",
    "print(\"\\nTraining and evaluating models...\")\n",
    "results = {}\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(model_name, model, X_train, y_train, X_test, y_test, skf):\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='accuracy')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return model_name, {\n",
    "        'model': model.__class__.__name__,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'test_acc': acc\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"\\nTraining and evaluating models...\")\n",
    "results = {}\n",
    "\n",
    "# Parallelize the model training\n",
    "# results_list = Parallel(n_jobs=-1)(delayed(train_and_evaluate_model)(model, X_train, y_train, X_test, y_test, skf) for name, model in models.items())\n",
    "results_list = dict(Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(train_and_evaluate_model)(name, model, X_train, y_train, X_test, y_test, skf) \n",
    "    for name, model in models.items())\n",
    ")\n",
    "\n",
    "\n",
    "for name, result in results_list.items():\n",
    "    print(f\"\\n{name} Cross-validation Accuracy: {result['cv_mean']:.4f} (+/- {result['cv_std']:.4f})\")\n",
    "    print(f\"{name} Test Accuracy: {result['test_acc']:.4f}\")\n",
    "    results[name] = result['test_acc']\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "\n",
    "# for name, model in models.items():\n",
    "#     print(f\"\\nTraining {name}...\")\n",
    "#     cv_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='accuracy')\n",
    "#     print(f\"{name} Cross-validation Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     acc = accuracy_score(y_test, y_pred)\n",
    "#     print(f\"{name} Test Accuracy: {acc:.4f}\")\n",
    "#     results[name] = acc\n",
    "\n",
    "# Display models ranked by test accuracy\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nModels ranked by test accuracy:\")\n",
    "for name, acc in sorted_results:\n",
    "    print(f\"{name}: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a00fad03-e99a-4608-bb5e-7ee3c4f79046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameter Tuning for Extra Trees...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best Parameters for Extra Trees: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Evaluating Best Extra Trees Model...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.75      0.72      1767\n",
      "           0       0.76      0.67      0.71      1768\n",
      "           1       0.64      0.66      0.65      1768\n",
      "\n",
      "    accuracy                           0.69      5303\n",
      "   macro avg       0.70      0.69      0.69      5303\n",
      "weighted avg       0.70      0.69      0.69      5303\n",
      "\n",
      "Accuracy Score: 0.6932\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Hyperparameter Tuning for Best Model\n",
    "# ================================\n",
    "\n",
    "# Let's assume XGBoost performed the best\n",
    "best_model_name = sorted_results[0][0]\n",
    "print(f\"\\nHyperparameter Tuning for {best_model_name}...\")\n",
    "\n",
    "if best_model_name == 'XGBoost':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [6, 10],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'subsample': [0.8, 1],\n",
    "        'colsample_bytree': [0.8, 1]\n",
    "    }\n",
    "    model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "elif best_model_name == 'LightGBM':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [6, 10, -1],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'num_leaves': [31, 50],\n",
    "        'subsample': [0.8, 1],\n",
    "        'colsample_bytree': [0.8, 1]\n",
    "    }\n",
    "    model = LGBMClassifier()\n",
    "elif best_model_name == 'CatBoost':\n",
    "    param_grid = {\n",
    "        'iterations': [500, 1000],\n",
    "        'depth': [6, 10],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'l2_leaf_reg': [1, 3, 5]\n",
    "    }\n",
    "    model = CatBoostClassifier(verbose=0)\n",
    "else:\n",
    "    # For other models, use generic parameters\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    model = models[best_model_name]\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                           cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Parameters for {best_model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate the best model\n",
    "print(f\"\\nEvaluating Best {best_model_name} Model...\")\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_.astype(str)))\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40045e71-2950-495d-b261-a67a93116eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting outcome for a new match...\n",
      "Predicted Outcome: Home Win\n",
      "\n",
      "Notebook execution completed successfully.\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   2.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  10.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   9.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   7.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  10.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   4.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   9.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   6.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   5.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  10.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   6.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   9.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   5.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   4.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  10.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   9.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  14.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  10.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   4.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   2.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   6.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   6.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  12.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  10.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   5.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   5.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   8.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  13.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   2.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   2.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  10.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   4.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   9.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   9.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   6.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   6.8s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# Prediction on New Data\n",
    "# ================================\n",
    "\n",
    "# For demonstration, let's predict the outcome of the next match in the dataset\n",
    "print(\"\\nPredicting outcome for a new match...\")\n",
    "\n",
    "# Assuming the next match is at index -1\n",
    "new_match = match_cleaned.iloc[-1]\n",
    "\n",
    "# Prepare the feature vector\n",
    "new_match_features = new_match[existing_features].values.reshape(1, -1)\n",
    "new_match_scaled = scaler.transform(new_match_features)\n",
    "\n",
    "# Predict using the best model\n",
    "prediction = best_model.predict(new_match_scaled)\n",
    "result = label_encoder.inverse_transform(prediction)[0]\n",
    "outcome = {1: 'Home Win', 0: 'Draw', -1: 'Away Win'}\n",
    "\n",
    "print(f\"Predicted Outcome: {outcome[result]}\")\n",
    "\n",
    "# ================================\n",
    "# Conclusion\n",
    "# ================================\n",
    "\n",
    "print(\"\\nNotebook execution completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d681a-ec1a-46fa-9fe9-fd2945ac4d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
